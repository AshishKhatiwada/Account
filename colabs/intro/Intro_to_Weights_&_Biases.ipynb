{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshishKhatiwada/Account/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install libraries\n",
        "!pip install transformers datasets sentencepiece accelerate torch pandas nltk\n",
        "\n",
        "# Step 2: Import packages\n",
        "import pandas as pd\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Step 3: Upload your dataset\n",
        "from google.colab import files\n",
        "files.upload()  # Upload your 'english_kriol_dataset.csv'\n",
        "\n",
        "# Step 4: Load and Clean Dataset\n",
        "df = pd.read_csv('english_kriol_dataset.csv')\n",
        "\n",
        "# Basic Cleaning\n",
        "df = df.dropna(subset=['english', 'kriol'])\n",
        "df['english'] = df['english'].str.strip()\n",
        "df['kriol'] = df['kriol'].str.strip()\n",
        "df = df[(df['english'] != \"\") & (df['kriol'] != \"\")]\n",
        "df = df[df['english'].str.split().str.len() <= 50]\n",
        "df = df[df['kriol'].str.split().str.len() <= 50]\n",
        "\n",
        "# Save cleaned dataset\n",
        "df.to_csv('cleaned_english_kriol_dataset.csv', index=False)\n",
        "\n",
        "# Step 5: Load cleaned dataset into Hugging Face Dataset\n",
        "dataset = load_dataset(\"csv\", data_files=\"cleaned_english_kriol_dataset.csv\")\n",
        "\n",
        "# Step 6: Load mBART model and tokenizer\n",
        "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name, src_lang=\"en_XX\")\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Step 7: Preprocess function\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(examples['english'], truncation=True, padding=\"max_length\", max_length=64)\n",
        "    targets = tokenizer(examples['kriol'], truncation=True, padding=\"max_length\", max_length=64)\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Step 8: Training Arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results_mbart_kriol\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=1,\n",
        "    save_total_limit=1,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "# Step 9: Trainer setup\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"train\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Step 10: Fine-tune model\n",
        "trainer.train()\n",
        "\n",
        "# Step 11: Save Fine-tuned Model\n",
        "trainer.save_model(\"./fine_tuned_mbart_kriol\")\n",
        "!zip -r fine_tuned_mbart_kriol.zip ./fine_tuned_mbart_kriol\n",
        "files.download(\"fine_tuned_mbart_kriol.zip\")\n",
        "\n",
        "# Step 12: Translate example English â†’ Kriol\n",
        "def translate_mbart_kriol(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    forced_bos_token_id = tokenizer.convert_tokens_to_ids(\"en_XX\")  # Staying within English since Kriol is not natively supported\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        forced_bos_token_id=forced_bos_token_id,\n",
        "        max_length=128\n",
        "    )\n",
        "    return tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "\n",
        "# Test Translation\n",
        "print(\"English: How are you?\")\n",
        "print(\"Kriol:\", translate_mbart_kriol(\"How are you?\"))\n"
      ],
      "metadata": {
        "id": "BXU3yblG9BIy",
        "outputId": "2deaa2d3-c98a-4a98-ff51-fa3d90880265",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary extracted successfully: 0 entries found\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}